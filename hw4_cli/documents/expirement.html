<html>
<head></head>
<body>

<h1>Expiremental results</h1>

<h2>Fibonacci</h2>
<br>
<br>
<table border="1" style="width:100%">
    <tbody><tr>
        <td><b>Scenario Type</b></td>
        <td>c = 1</td>
        <td>c = 8</td>
        <td>Parallel efficieny</td>
    </tr>
    <tr>
        <td>communication latency amelioration turned off; single worker thread</td>
        <td>11809.445141 ms</td>
        <td>3109.296521 ms</td>
        <td><a href="fib_off_1.png">View Plot</a></td>

    </tr>
    <tr>
        <td>communication latency amelioration turned off; multiple worker threads</td>
        <td>11882.573455 ms</td>
        <td>3120.189158 ms</td>
        <td><a href="fib_off_mult.png">View Plot</a></td>
    </tr>
    <tr>
        <td>communication latency amelioration turned on; single worker thread</td>
        <td>962.980542 ms</td>
        <td>3183.548287 ms</td>
        <td><a href="fib_on_1.png">View Plot</a></td>
    </tr>
    <tr>
        <td>communication latency amelioration turned on; multiple worker threads</td>
        <td>908.017483 ms</td>
        <td>660.682007 ms</td>
        <td><a href="fib_on_mult.png">View Plot</a></td>
    </tr>
    </tbody></table>

<h2>TSP</h2>
<br>
<br>
<table border="1" style="width:100%">
    <tbody><tr>
        <td><b>Scenario Type</b></td>
        <td>c = 1</td>
        <td>c = 8</td>
        <td>Parallel efficieny</td>
    </tr>
    <tr>
        <td>communication latency amelioration turned off; single worker thread</td>
        <td>90303.971458 ms</td>
        <td>12443.779384 ms</td>
        <td><a href="tsp_off_1.png">View Plot</a></td>

    </tr>
    <tr>
        <td>communication latency amelioration turned off; multiple worker threads</td>
        <td>20127.75843 ms</td>
        <td>4154.945843 ms</td>
        <td><a href="tsp_off_mult.png">View Plot</a></td>
    </tr>
    <tr>
        <td>communication latency amelioration turned on; single worker thread</td>
        <td>90108.397886 ms</td>
        <td>11539.21756 ms</td>
        <td><a href="tsp_on_1.png">View Plot</a></td>
    </tr>
    <tr>
        <td>communication latency amelioration turned on; multiple worker threads</td>
        <td>19531.294105 ms</td>
        <td>4244.49093 ms</td>
        <td><a href="tsp_on_mult.png">View Plot</a></td>
    </tr>
    </tbody></table>

<h2>Experiment Discussion</h2>

For every experiment but one, the client's execution time was significantly smaller for 8 computers compared to 1 computer.
For Fibonacci, it did not matter how many threads a computer was running; a time reduction was instead seen when amelioration
was turned on. It makes sense that we see amelioration pay off, as the Space/Client no longer have to wait for each other to send
tasks/results. However, for TSP, it was the complete opposite: amelioration did not shave off time, however multiple worker threads did.
A possible explanation is that computing a TSP task requires much more computational power than a FIB task, therefore having many threads
helped out.

The Parallel Efficiency graphs for FIB show that the efficiency drops when using 8 computers. A possible explanation is that while we do have
much more computational power at our disposal with 8 computers, if one computer is a little slower, the rest of the computers/space are now waiting for it.
The Efficiency for TSP (C=8) was better than for FIB, sometimes differing only by %2 from C=1.

    
    <h2>Future Infrastructure Improvements</h2>
    <h3>Caching at Decomposition Level</h3>
    <p>Currently we have basic caching implemented for tasks, but the caching only currently works after decomposition. For example for Fib(2), while the values of Fib(1)=1, Fib(0)=0 and Add(1,0)=1 are cached when Fib(2) is encountered again it would still do the decomposition.
        Ideally in the future we would keep track of the origin node of a decomposition and be able to cache the resulting value after the subgraph merges again. This would allow us to radically cut the execution of problems like Fib that benifit from caching.</p>
        
        <h3>Smarter Scheduling</h3>
        <p>Currently we schedule tasks as Computer become available. At a given time a task is scheduled on at most one machine, and is rescheduled only in the event of failure. This works alright if task sizes are realitivley even and computers are of roughly equal power. However in the real world this is unlikley to be the case.
            In the event that a computer is slow it could hog a task, while other faster omputers are available and idle. This is exaserbated with our implementation of prefetching as a Computer can have multiple assigned tasks while other computers are idle. In the future we could implement task stealing so that if a computer is idle or about to become so it can get tasks from overloaded computers. We can also dispatch multiple copies of tasks (at the end of execution) to minimize execution tail bottleneck. Overall smarter prioritization of execution order might be prudent, as we could in the future prioritize on decomposition tasks over composition at the start of computation to maximize the effet of parallism.</p>
            
        <h3>Shared State</h3>
        <p>Another optimization is sharing state variables between computers. We will do this for HW5. For types of computations that search over a problem space it might be benificial to keep a shared state of best answer found so far. For ecample in TSP the best length encountered could allow other computers to shorcut searches where partial results already exceed this bound.</p>
        
        <h3>Network of Spaces</h3>
        <p>Lastly, while we currently differentiate between spaces and computers, in the future we could treat spaces as forms of computers that form a network of spaces. At each level spaces can delegate some of the work to connected spaces while doing a portion of the computation themselves. While this aproach is not likley to help with latencey, it would make the system more scalable.</p>
</body>
</html>